{"componentChunkName":"component---src-templates-blog-post-js","path":"/posts/useful_noise/","result":{"data":{"markdownRemark":{"html":"<style type=\"text/css\">\n.card {\n  width: 37em;\n  margin: 0 auto; /* Added */\n  float: none; /* Added */\n  margin-bottom: 10px; /* Added */\n  text-align: center;\n  background-color: #FFF8DC;\n  /* Add shadows to create the \"card\" effect */\n  box-shadow: 0 4px 4px 0 rgba(0, 0, 0, 0.2);\n  transition: 0.3s;\n}\n\n/* On mouse-over, add a deeper shadow */\n.card:hover {\n  box-shadow: 0 8px 8px 0 rgba(0, 0, 0, 0.2);\n}\n\n/* Add some padding inside the card container */\n.container {\n  padding: -0.5px 5px;\n}\n\n@media screen and (max-width: 900px) {\n  .card {\n    width: 30em;\n  }\n}\n\n</style>\n<p>Data-powered applications are the essence of the AI era, and the number of these apps is believed to increase for the decades to come. From search engines to recommendations systems, images and videos applications to text understanding applications, etc, data-apps have taken a huge place in the lives of many people. Teams (data scientists, software engineers and subject-matter experts) in charge of building these apps are usually eager to collect any type of data and use them for analysis, prediction models and sometimes sharing (for data science competitions purposes for example). However, breaches and data leakages have been documented in the past years and have shown the intensive data collection is not totally harmless to the end-users of these apps.</p>\n<p>Existing scientific litterature had proven differential privacy to be an effective way to ensure users's privacy in data collection and/or machine learning analysis. The technique has been pionneered by Cynthia Dwork since the early 2000's and has received a lot of attention in the community ever since. Some of the big players in AI ground (Apple, Google, Microsoft, Samsung) have productionize it to preserve users privacy. This technique can clearly help fill the trust gap between curators and users. <strong><em>Applying this idea, I will try to preserve users information in a database while building machine learning model.</em></strong></p>\n<h3>Experimental data</h3>\n<p>Between 2002-2004, researchers at Columbia Business School [1] collected information from experimental speed dating events. The participants have 4 minutes to chat with every other participant of the opposite sex (we are quite far from the <strong>Tinder-world</strong> we know now ðŸ˜…). The organizers collected many attributes such as match decision(if they want to see the other partner after the 4 four minutes talk), demographics, self-perception attributes, other perception attributes, beliefs, attractiveness, sincerity, ambition, intelligence, fun, etc. The <strong><em>\"match\"</em></strong> variable is the target variable.</p>\n<p align=\"center\">\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 800px; \">\n      <a class=\"gatsby-resp-image-link\" href=\"/static/58dcb5794aa8a86db9712e2e81d263e8/9c701/dataset.png\" style=\"display: block\" target=\"_blank\" rel=\"noopener\">\n    <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 61%; position: relative; bottom: 0; left: 0; background-image: url(&apos;data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAMCAYAAABiDJ37AAAACXBIWXMAABYlAAAWJQFJUiTwAAABzElEQVQoz22T6Y6rMAyFef/Hayu1VRegLTsUAoSdnsnxTLiqdH9YNo4Tf3EOThgECNwLkqeLMAwQxzGezydqVaMoCnieh6ZpkOe55F+vl9jj8UDAvcYYM8d1Rzc1ssCDymPUZYFWa1RVhbqucT6fcTgcEEURrteraRhiGAZoUzOOI7quk5g5Wtu2cObRFFQFRt1g7DTWdZVienY8nU6yieQk/nw+mOdZ/LqsWJZFaj/rb14IiziAKjKody6EyhCy2+VywW63k+77/V4o+76XNTahtzGNoxHCKovR1RX6tsZkuvAq0zQJoeu6cn3GaZoKGddIRDpSkZDGvNO1ZuDRSwjtDEnIjiQkGSk4+CRJNkJS2xkyZ/PO2Hdoyxx9o4RwNF1YyDnykOPxKIS+5yOOYiHiYaRhDY2x/XYG8xDvJIRWb+haYTBJzoKb7vf79iiky7JsO5DeHsLY5p3WkKXBA1WRyqM0BrssS8G3B3J21CPlY69Gbx/j68r/ZGMepNeYzaDZiQO3V1ZKiYAp7m/ZLFLH+Fs2iZHN+082rd4IrWz4zVemsDlfjoT+v7KZhh6lOZCUIptp3v4E3/dxu92kkL8Wr27lQW/naGXDPT+gCpN7WQRXLwAAAABJRU5ErkJggg==&apos;); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"dataset of the experimentation\" title=\"dataset of the experimentation\" src=\"/static/58dcb5794aa8a86db9712e2e81d263e8/5a190/dataset.png\" srcset=\"/static/58dcb5794aa8a86db9712e2e81d263e8/772e8/dataset.png 200w,\n/static/58dcb5794aa8a86db9712e2e81d263e8/e17e5/dataset.png 400w,\n/static/58dcb5794aa8a86db9712e2e81d263e8/5a190/dataset.png 800w,\n/static/58dcb5794aa8a86db9712e2e81d263e8/c1b63/dataset.png 1200w,\n/static/58dcb5794aa8a86db9712e2e81d263e8/29007/dataset.png 1600w,\n/static/58dcb5794aa8a86db9712e2e81d263e8/9c701/dataset.png 1774w\" sizes=\"(max-width: 800px) 100vw, 800px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n  </a>\n    </span>\n</p>\n<center><i>Fig 1: Experimental dataset</i></center>\n<p></p>\n<p>So for this exceprt of the dataset, the first indiviudal (who is a female) have interactions with 10 males during the speed dating events. She is 21 and she has accepted to meet (target variable <strong><em>match</em></strong>) 4 out of the 10 males, after the events. She is an asian american and has met with 8 caucasian american, 1 asian american and 1 hispanic american.</p>\n<p>The data is taken from <a href=\"https://www.openml.org/\">OpenML</a> and can be access <a href=\"https://www.openml.org/d/40536\">here</a></p>\n<p>The dataset has 8370 rows for 123 columns. There are 1380 matches meaning 16.5% of the total dataset. Out of the 123 columns, there are a lot of <em>duplicate</em> columns, i.e the discretized version of each column. For example, from the <em>Fig 1</em> above, the 4th column is the difference of the age between the age of a female participant and a male participant that she has met during the event. The 5th column reprsent the discretized version of this difference. I decided to drop all of the discetized versions of columns. It then remains only 64 columns in the whole dataset.</p>\n<h3>Differential Privacy in a nutshell</h3>\n<p>Differential Privacy (DP) is a mechanism designed to protect information of individuals through a randomized process. Concretely, it introduces noise into the data in order to retain information from an attacker (any individual who might want to access information that is not intended to be disclosed).</p>\n<p>The promise of DP is to ensure the plausible deniability of any user's information who shares his/her data. This means, the results of a data analysis will not change if any individual from the database have not been part of the dataset. For an introduction to the concept of DP, please take a look at <a href=\"https://medium.com/@capgemini.invent.europe/differential-privacy-embedding-privacy-into-data-usage-f827f620f886\">my first post</a> on the subject. There exists two forms of DP:</p>\n<ul>\n<li>the (global) DP in which data is collected from every individuals and centralized into a single database (or server); the noise is then introduced in an aggregated result of a query, of the centralized data before publishing.</li>\n<li>the Local Differential Privacy (LDP) in which every individual sends a noisy data and holds the true data on their devices. The noisy data is centralized by the aggregator. This setting brings stronger privacy guarantees than the previous one as individuals do not have to trust the aggregator. The cost of applying LDP instead of the glovbal DP is to have a huge dataset so we can average out the local noises added to each individual-level information.</li>\n</ul>\n<p>For more advanced tasks such as modelization, there is a need to have a DP mechanism enforced into the algorithms. It has been demonstrated that some machine learning models especially deep learning models have the ability to memorized part of the training data. By a reverse-engineered process, one can recover the individuals' data from a deployed deep learning model. Here comes the notion of <strong>privacy preserving machine learning (PPML)</strong> that achieves a certain level of privacy within machine learning algorithms.</p>\n<p>One such approach of enforcing privacy into an advanced machine learning algorithm is <strong>DP-SGD (Differentially Private Stochastic Gradient Descent)</strong> designed by Abadi et al [2].</p>\n<h3>Main principle of DP-SGD</h3>\n<p>SGD is a core technique in deep learning algorithms. It is an optimization procedure where at each iteration the error between the model's predictions and the true labels is computed on a sample batch of the data; this error is computed on the derivatives (gradients) of each parameters and the parameters are updated in order to close the gap the true labels and the model's predictions . For a more understanding on the gradient descent, please see this <a href=\"https://ruder.io/optimizing-gradient-descent/\">post</a> [3].</p>\n<div class=\"card\">\n  <div class=\"container\">\n    <h3>\n    <i>\n    The basic idea behind DP-SGD is to control the influence of \n    each training example during the training process\n    </i>\n    </h3>\n  </div>\n</div>\n<h2></h2>\nDP-SGD brings two modifications to the classical SGD:\n<ul>\n<li>gradients clipping : at each iteration, each individual gradient's <em>l2 norm</em> is clipped by a value C; that is if the <em>l2 norm</em> â‰¤ C, we keep the gradients, and if the <em>l2 norm</em> is > C we scale it down by a factor of <em>l2 norm</em> divided by C. Gradients' clipping is a very well know technique in deep learning community</li>\n<li>gradients random noise : a gaussian noise is sampled and added to every clipped gradients; this ensures the deniability of any individual in the training set as an adversary who has information about the model's parameters cannot recover any training data point.</li>\n</ul>\n<p>In an <a href=\"http://benocharlo.com/posts/patedp-sgd/\" target=\"_blank\">upcoming post</a>, I will explain in detail two PPML techniques : DP-SGD and PATE. Now let's move on to our formal modelisation.</p>\n<h3>Baseline Model</h3>\n<p>As a first step, I will build a classification model to determine the probability of a future match, based on the speed-dating data. I decided to go for neural nets using Tensorflow/Keras. I have made a two-step train/test split of the dataset. The frist step is an 70-30% division of the dataset. The second step is a split of the 70% part into 80-20% part each. This way, we have a train-validation-test sets for our modelization. Classical!</p>\n<p>Some processing are performed.</p>\n<p>The basic model I have built is a simple neural network with 3 dense hidden layers (64, 128 and 256 neurons) and 1 sigmoid activation layer as the output.</p>\n<p>The best model computation run on this dataset gives 87% in predictive accuracy. While, the main purpose of this post is not about improving the accuracy score of this dataset, we will try to achieve a 90% accuracy score for the match variable.</p>\n<h3>Learning privately from the data</h3>\n<p>Abadi et al, have designed a set of hyperparameters that can be tuned for learning privately using DP-SGD:</p>\n<ul>\n<li><em>l2_norm_clip</em>: this is the clipping factor that we have talked about earlier. Any gradient is not allowed to exceed a proportion of this factor.</li>\n<li><em>noise_multiplier</em> : this parameter is the level of noise we add to each clipped gradient. The more noise we add to the gradients, the more private and the lesser accurate the model is.</li>\n<li><em>microbatches</em> : for a more private learning, the gradients should be clipped one by one. But this implies a computational overhead. A solution to reduce the computational overhead is to increase the size of microbatches, meaning grouping more gradients and clipping the averaged gradient. The authors of the DP-SGD have designed the bacth size to be evenly divided by the <em>microbatches</em>.</li>\n<li><em>learning_rate</em> : this is the usual udpate parameter in SGD method. A lower learning rate helps converge but the training procedure is slower.</li>\n</ul>\n<p>To implement the private learning, I have used (well, you guessed it ðŸ˜‰) <a href=\"https://github.com/tensorflow/privacy\">tensorflow-privacy</a></p>\n<h3>Measure of the privacy guarantee</h3>\n<h3>References</h3>\n<ol>\n<li>Raymond Fisman, Sheena S. Iyengar, Emir Kamenica, Itamar Simonson <a href=\"https://doi.org/10.1162/qjec.2006.121.2.673\"><em>Gender Differences in Mate Selection: Evidence From a Speed Dating Experiment</em></a>, The Quarterly Journal of Economics, Volume 121, Issue 2, 1 May 2006, Pages 673â€“697</li>\n<li>Martin Abadi, Andy Chu, Ian Goodfellow, H. Brendan McMahan, Ilya Mironov, Kunal Talwar, Li Zhang <a href=\"https://arxiv.org/pdf/1607.00133.pdf\"><em>Differential Private Deep Learning</em></a></li>\n<li>Sebastian Ruder, <a href=\"https://ruder.io/optimizing-gradient-descent/\"><em>An overview of gradient descent optimization algorithms</em></a></li>\n<li>4.</li>\n<li>5.</li>\n</ol>","frontmatter":{"title":"Useful noise"},"excerpt":"Data-powered applications are the essence of the AI era, and the number of these apps is believed to increase for the decades to come. Fromâ€¦"}},"pageContext":{"slug":"/posts/useful_noise/"}},"staticQueryHashes":["199442291","2997033868","3361508366"]}